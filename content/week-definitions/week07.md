---
title: "Week 7: Deep Learning Basics"
date: 2025-10-27
description: "Deep learning basics, MLPs, Feed-forward networks, Exercise 4 distributed"
lecture_slides: 
  - title: "Lecture slides, week 7"
    link: "/ada-course-materials/advanced_data_analytics_2025/lectures/lecture_7/slides/Advanced_Data_Analytics_2025_lecture_7.pdf"
ta_slides: []
assignment:
  title: "Exercise 4"
  link: "/ada-course-materials/advanced_data_analytics_2025/exercises/Exercise_4/Problem_set_4.pdf"
  description: "Distribution of Exercise sheet 4"
examples:
  - title: "Introduction to Tensorflow, applied to supervised machine learning problems"
    link: "/ada-course-materials/advanced_data_analytics_2025/lectures/lecture_7/demo/03_Gentle_DNN.ipynb"
  - title: "Multiple Demo Files"
    link: "/ada-course-materials/advanced_data_analytics_2025/lectures/lecture_7/demo/"
references:
  - title: "Deep Learning Materials"
    link: "/ada-course-materials/advanced_data_analytics_2025/lectures/lecture_7/"
    description: "Discussion of the previous exercises (with TA)"
---

## Week 7: Deep Learning Basics

### Learning Objectives
- Understand the fundamentals of neural networks and deep learning
- Learn about multi-layer perceptrons (MLPs) and their architecture
- Master feed-forward networks and their training process
- Understand stochastic gradient descent (SGD) and backpropagation
- Recognize overfitting issues and regularization techniques
- Apply deep learning to real-world problems

### Topics Covered
- **Deep Learning Basics**: Neural network fundamentals and motivation
- **Multi-layer Perceptron (MLP)**: Architecture and design principles
- **Feed-forward Networks**: Information flow and network topology
- **Network Training**: Stochastic Gradient Descent (SGD) optimization
- **Error Backpropagation**: Algorithm for computing gradients
- **Overfitting Prevention**: Regularization techniques and best practices
- **Introduction to TensorFlow**: Applied to supervised learning problems

### Schedule
- **Lecture**: Monday, October 27, 2025 (10:15 - 12:00)
- **Practice Session**: Monday, October 27, 2025 (16:30 - 18:00)
- **TA Session**: Discussion of exercises and neural network implementations

### Key Concepts
- **Perceptron**: Single neuron model and limitations
- **Universal Approximation Theorem**: Theoretical foundation of neural networks
- **Activation Functions**: ReLU, sigmoid, tanh, and their properties
- **Loss Functions**: Mean squared error, cross-entropy for different tasks
- **Optimization**: Gradient descent variants and learning rates
- **Regularization**: Dropout, weight decay, early stopping

### Practical Skills
- Building neural networks from scratch
- Implementing backpropagation algorithm
- Using TensorFlow for deep learning tasks
- Network architecture design and hyperparameter tuning
- Debugging and troubleshooting neural networks

### Assignments
- **Exercise 4**: Distributed this week - Neural network implementation
- Practice building and training MLPs
- Explore different activation functions and architectures

### Further Reading
- Deep Learning textbook chapters on MLPs and backpropagation
- TensorFlow documentation and tutorials